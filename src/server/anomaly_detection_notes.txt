================================================================================
ANOMALY DETECTION MODULE - DETAILED NOTES & DOCUMENTATION
================================================================================
This file contains all detailed comments, mathematical foundations, and design
rationale extracted from anomaly_detection.py for reference.

================================================================================
1. OVERVIEW
================================================================================
Anomaly detection module for federated learning. Identifies malicious client
updates before they corrupt the global model.

Attack types defended against:
  1. Scaling Attack: Amplify update magnitude for disproportionate influence
  2. Sign-Flip Attack: Reverse gradient direction to push model backward
  3. Noise Injection: Add random noise to destroy model convergence
  4. Coordinated Attack: Multiple malicious clients collude

References: Baruch et al. (2019), Shejwalkar & Houmansadr (2021)

METHODS AVAILABLE (kept: GradientNormDetector + ReputationSystem):
  1. GradientNormDetector - z-score + IQR on gradient L2 norms
  2. CosineSimilarityDetector - cosine similarity vs reference direction
  3. ClusteringDetector - DBSCAN on update vectors
  4. ReputationSystem - tracks client trust history over time

================================================================================
2. GRADIENT NORM DETECTOR
================================================================================
Detects anomalies based on gradient norm analysis.

INTUITION:
  In a scaling attack, malicious client amplifies their update to have
  disproportionate influence on the global model. This results in an
  abnormally large gradient norm.

  Example:
    Honest update norm: ~1.0
    Malicious (scaling) update norm: ~100.0

THRESHOLD METHODS:
  1. Z-score: Flag if |z| > threshold (typically 2-3)
     z = |norm - mean| / std
  2. IQR: Flag if outside [Q1 - k*IQR, Q3 + k*IQR]
     where IQR = Q3 - Q1, k typically = 1.5
  3. Percentile: Flag if in extreme percentiles

Combined: is_anomaly = z_anomaly OR iqr_anomaly
Anomaly score (0-1): z_score / (2 * z_threshold)

WEAKNESS:
  Sophisticated attackers can normalize their updates to have reasonable
  norms while still being malicious in direction. (Solved by combining
  with reputation tracking.)

PARAMETERS:
  z_score_threshold (default 2.5): How many std devs to allow
  use_iqr (default True): Whether to also use IQR method
  iqr_multiplier (default 1.5): IQR multiplier for bounds

L2 NORM:
  Formula: ||gradient||_2 = sqrt(sum(g_i^2))
  Why L2: Most common in ML, corresponds to Euclidean distance,
  penalizes large values more heavily.

================================================================================
3. COSINE SIMILARITY DETECTOR (removed from code, kept for reference)
================================================================================
Detects anomalies based on update direction alignment.

INTUITION:
  In model poisoning attacks, the malicious update points in a direction
  that degrades the model. By comparing the direction (angle) of each
  update to the expected direction, we can detect these attacks.

  Unlike gradient norm detection, cosine similarity is invariant to the
  magnitude of the update. This catches sophisticated attackers who
  normalize their update to have a reasonable norm.

Formula:
  cos_sim = (a . b) / (||a|| x ||b||)
  Result in [-1, 1]: 1 = same direction, -1 = opposite, 0 = orthogonal

PARAMETERS:
  similarity_threshold (default 0.0): Minimum acceptable similarity

WEAKNESS:
  Needs a reliable reference direction (chicken-and-egg problem â€” you
  need to know the "correct" direction to compare against, but that's
  what you're trying to compute).

================================================================================
4. CLUSTERING DETECTOR (removed from code, kept for reference)
================================================================================
Detects coordinated attacks using DBSCAN clustering.

INTUITION:
  In a coordinated attack, multiple malicious clients send similar bad
  updates, hoping to overwhelm honest clients. By clustering all updates,
  we can identify these groups:
    - Large cluster = likely honest majority
    - Small clusters = potentially coordinated attacks
    - Noise points = individual outliers

DBSCAN is ideal because:
  - Doesn't require specifying number of clusters
  - Naturally identifies outliers (noise points)
  - Handles arbitrary cluster shapes

Auto-eps: median of nearest-neighbor distances * 1.5

NOTE: This was redundant with AWTM's own DBSCAN clustering, so it
was removed from the active detection pipeline to avoid double-clustering.

================================================================================
5. REPUTATION SYSTEM
================================================================================
Manages client reputation scores over time.

INTUITION:
  A client's reputation reflects their historical behavior. Honest clients
  build good reputation over time, while malicious clients accumulate
  negative history.

REPUTATION UPDATE FORMULA:
  reputation_new = reputation_old * decay + reward
  Where:
    decay in [0, 1]: How much to forget old reputation (default 0.9)
    reward in [-1, 1]: Based on current behavior

REWARD COMPUTATION:
  - Flagged as malicious: reward = -flag_penalty (default -0.2)
  - Not flagged: reward = +good_reward (default +0.05)
  - Reputation clamped to [0, 1]

THRESHOLD FOR EXCLUSION:
  Clients with reputation below min_reputation (default 0.3) are:
    - Given lower weight in aggregation
    - Flagged for closer scrutiny
    - Potentially excluded from training

PERSISTENT OFFENDER DETECTION:
  Clients flagged consecutively >= consecutive_threshold (default 3)
  times are identified as persistent offenders.

DEFENDING AGAINST REPUTATION ATTACKS:
  Sophisticated attackers might:
    - Build good reputation, then attack (sleeper agents)
    - Collude to boost each other's reputation
  Our defenses:
    - Reputation decay ensures recent behavior matters more
    - Consecutive flag counter catches sudden behavior changes

CLIENT PROFILE TRACKING:
  Per-client data stored:
    - client_id, reputation, total_updates, flagged_updates
    - avg_gradient_norm, norm_history (last 100 values)
    - last_update_time, consecutive_flags

================================================================================
6. ANOMALY DETECTOR (ORCHESTRATOR)
================================================================================
Main anomaly detection system that combines detection methods.

DESIGN PHILOSOPHY:
  1. Defense in Depth: Multiple detectors catch different attack types
  2. Conservative: Better to flag suspicious than miss attacks
  3. Adaptive Thresholds: Learn from observed data
  4. Explainability: Provide reasons for each flag

DETECTION PIPELINE (simplified):
  1. Compute all gradient norms
  2. For each update: run norm check + reputation check
  3. Combine scores with weights
  4. Flag if combined score > threshold
  5. Update reputation

SCORING:
  combined_score = norm_weight * norm_score + reputation_weight * reputation_score
  is_malicious = combined_score > threshold

API (must match server.py expectations):
  detect_batch(updates) -> List[DetectionResult]
  get_reputation_scores() -> Dict[str, float]
  get_malicious_clients() -> List[str]

================================================================================
END OF NOTES
================================================================================
